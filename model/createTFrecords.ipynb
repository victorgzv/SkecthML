{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images:  149\n",
      "Total number of xmls:  149\n",
      "Training data lenght:  624\n",
      "Validation data lenght  157\n",
      " Training TFRecords created\n",
      "Validation TFRecords created \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Victor Gonzalez - D16123580 - DT228\n",
    "|DATA PREPARATION|\n",
    "Once the 149 images have been manually labelled this script prepares the images and\n",
    "their annotations for training the model.\n",
    "The following data must be converted to TensorFlow format in order to train the model.\n",
    "The steps are:\n",
    "1. Convert XML annotations into a pandas Dataframe.\n",
    "2. Split dataframe into training and testing data.\n",
    "3. Convert each dataframe into csv files.\n",
    "4. Covert each csv file into TensorFlow records format.\n",
    "'''\n",
    "# import the necessary packages:\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mimg\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "from models.research.object_detection.utils import dataset_util\n",
    "from models.research.object_detection.utils import label_map_util\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import namedtuple, OrderedDict\n",
    "#*******************************************************************************************************************\n",
    "# A function to parse the xml files\n",
    "def read_xml(file):\n",
    "    data = [] #array to store extracted values from the xml files\n",
    "    # for each file\n",
    "    for xml in file:\n",
    "        # Geting xml\n",
    "        tree = ET.parse(xml)\n",
    "        # Getting the root of the xml file\n",
    "        root = tree.getroot()\n",
    "        # Getting every xml tag that contains objects\n",
    "        for x in root.findall('object'):\n",
    "            filename = root.find('filename').text#Name of the image\n",
    "            width =  int((root.find('size')).find('width').text)#Width of the image\n",
    "            height = int((root.find('size')).find('height').text)#Name of the image\n",
    "            # coordinate of each labelled object in the xml\n",
    "            bounding_box = x.find('bndbox')\n",
    "            xmin = float(bounding_box.find('xmin').text)\n",
    "            xmax = float(bounding_box.find('xmax').text)\n",
    "            ymin = float(bounding_box.find('ymin').text)\n",
    "            ymax = float(bounding_box.find('ymax').text)\n",
    "            label =  x.find('name').text\n",
    "            #Add values to array data\n",
    "            data.append((filename, width, height, label, xmin, ymin, xmax, ymax))\n",
    "    # Creating dataframe\n",
    "    columns_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    # Dataframe is created with the array of values extracted from each xml filw and the column names above\n",
    "    df = pd.DataFrame(data=data, columns=columns_name)\n",
    "    return df\n",
    "#*******************************************************************************************************************\n",
    "# Function to create group in the dataframe. Each image contains 1 or more objects\n",
    "def make_groups(df, field=None):\n",
    "    if field==None:\n",
    "        field = 'filename'\n",
    "\n",
    "    data = namedtuple('object', ['filename', 'info'])#group object by image and its annotations\n",
    "    grouped = df.groupby(field)\n",
    "    grouped_data = []\n",
    "    for filename, x in zip(grouped.groups.keys(), grouped.groups):\n",
    "        grouped_data.append(data(filename, grouped.get_group(x)))\n",
    "    return grouped_data\n",
    "#*******************************************************************************************************************\n",
    "# Convert data into TF records (TensorFlow format)\n",
    "def convert_to_tf(group, img_path, label_map_dict):\n",
    "      # TensorFlow function to read images.\n",
    "      with tf.gfile.GFile(os.path.join(img_path, '{}'.format(group.filename)), 'rb') as f:\n",
    "          img_file = f.read()\n",
    "      # Encode to bytes\n",
    "      encoded_img = io.BytesIO(img_file)\n",
    "      # Read the image using PIL\n",
    "      img = Image.open(encoded_img)\n",
    "      width, height = img.size#get size of each image\n",
    "      # Encode the name of the img file\n",
    "      filename = group.filename.encode('utf8')\n",
    "      # format of the image\n",
    "      img_format = b'jpg'   # Image in bytes\n",
    "      # Dvariables for features of the TF records\n",
    "      xmins = []\n",
    "      xmaxs = []\n",
    "      ymins = []\n",
    "      ymaxs = []\n",
    "      classes_text = []\n",
    "      classes = []\n",
    "      # Reading each group of images (image and its annotations)\n",
    "      for index, row in group.info.iterrows():\n",
    "          xmins.append(row['xmin'] / width)\n",
    "          xmaxs.append(row['xmax'] / width)\n",
    "          ymins.append(row['ymin'] / height)\n",
    "          ymaxs.append(row['ymax'] / height)\n",
    "          classes_text.append(row['class'].encode('utf8'))\n",
    "          classes.append(label_map_dict[row['class']])\n",
    "\n",
    "      tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "          'image/height': dataset_util.int64_feature(height),\n",
    "          'image/width': dataset_util.int64_feature(width),\n",
    "          'image/filename': dataset_util.bytes_feature(filename),\n",
    "          'image/source_id': dataset_util.bytes_feature(filename),\n",
    "          'image/encoded': dataset_util.bytes_feature(img_file),\n",
    "          'image/format': dataset_util.bytes_feature(img_format),\n",
    "          'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "          'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "          'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "          'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "          'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "          'image/object/class/label': dataset_util.int64_list_feature(classes),}))\n",
    "\n",
    "      return tf_example\n",
    "#*******************************************************************************************************************\n",
    "# Images folder\n",
    "img_path = 'sketchML/training_images/'\n",
    "# Label map contains the classes to be detected\n",
    "label_map_dict = label_map_util.get_label_map_dict('sketchML/label_map.pbtxt')\n",
    "# Using glob to read the images and xml contained in teh training images folder\n",
    "images = sorted(glob.glob('sketchML/training_images/*.jpg'))\n",
    "xmls = sorted(glob.glob('sketchML/training_images/*.xml'))\n",
    "#printing values\n",
    "print(\"Total number of images: \", len(images))\n",
    "print(\"Total number of xmls: \", len(xmls))\n",
    "#Call to function that reads the xml files\n",
    "df = read_xml(xmls)\n",
    "#Splitting dataframe into training a validation test 80% is training and 20% is validation data.\n",
    "train, valid = train_test_split(df, test_size=0.2, stratify=df['class'], random_state=111)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "# Converting both dataframes into CSV files\n",
    "train.to_csv('sketchML/training_data/train.csv')\n",
    "valid.to_csv('sketchML/training_data/valid.csv')\n",
    "\n",
    "print(\"Training data lenght: \", len(train))\n",
    "print(\"Validation data lenght \", len(valid))\n",
    "\n",
    "writer = tf.python_io.TFRecordWriter('sketchML/records/train.record')\n",
    "# Call to the group image and their data function\n",
    "img_groups = make_groups(train, field='filename')\n",
    "# Iterate groups to create tfrecords\n",
    "for group in img_groups:\n",
    "    tf_example = convert_to_tf(group, img_path, label_map_dict)\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "# closing tensorflow writting handle\n",
    "writer.close()\n",
    "print(\" Training TFRecords created\")\n",
    "\n",
    "# Preparing handle for TF records conversion\n",
    "writer = tf.python_io.TFRecordWriter('sketchML/records/valid.record')\n",
    "# Call to the group image and their data function\n",
    "img_groups = make_groups(valid, field='filename')\n",
    "# Iterate groups to create tfrecords\n",
    "for group in img_groups:\n",
    "    tf_example = convert_to_tf(group, img_path, label_map_dict)\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "# closing tensorflow writting handle\n",
    "writer.close()\n",
    "print(\"Validation TFRecords created \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
